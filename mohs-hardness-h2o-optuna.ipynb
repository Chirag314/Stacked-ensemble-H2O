{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60892,"databundleVersionId":6989718,"sourceType":"competition"},{"sourceId":6612067,"sourceType":"datasetVersion","datasetId":3815527}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"font-family: 'JetBrains Mono'; font-weight: bold; font-size: 125%; color: #4A4B52; text-align: center\">Playground Series S3E25 - Mohs Hardness</p>","metadata":{}},{"cell_type":"code","source":"# %load ../utils/config.py\n!pip install -q kaleido\nimport glob\nimport operator\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport warnings\nfrom array import array\nfrom collections import defaultdict, namedtuple\nfrom copy import copy\nfrom functools import partial, singledispatch\nfrom itertools import chain, combinations, product\nfrom pathlib import Path\nfrom time import strftime\n!pip install --upgrade scikit-learn\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport scipy.stats as stats\nimport seaborn as sns\nimport shap\nfrom colorama import Fore, Style\nfrom IPython.display import HTML, Image, display_html\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom plotly.subplots import make_subplots\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.spatial.distance import squareform\nfrom sklearn import clone\nfrom sklearn.base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    MetaEstimatorMixin,\n    OneToOneFeatureMixin,\n    TransformerMixin,\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.discriminant_analysis import StandardScaler\nfrom sklearn.ensemble import (\n    GradientBoostingRegressor,\n    IsolationForest,\n    RandomForestRegressor,\n)\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.linear_model import LogisticRegression, SGDOneClassSVM\nfrom sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding\nfrom sklearn.metrics import (\n    confusion_matrix,\n    median_absolute_error,\n    roc_auc_score,\n    roc_curve,\n)\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n)\nfrom sklearn.neighbors import KNeighborsRegressor, LocalOutlierFactor\nfrom sklearn.pipeline import FunctionTransformer, make_pipeline, make_union\nfrom sklearn.preprocessing import MinMaxScaler, PowerTransformer, RobustScaler\nfrom sklearn.svm import SVC, SVR, LinearSVR\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.utils.validation import check_array, check_is_fitted\nfrom xgboost import XGBClassifier\n\n# Environment\nON_KAGGLE = os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n\n# Colorama settings.\nCLR = (Style.BRIGHT + Fore.BLACK) if ON_KAGGLE else (Style.BRIGHT + Fore.WHITE)\nRED = Style.BRIGHT + Fore.RED\nBLUE = Style.BRIGHT + Fore.BLUE\nCYAN = Style.BRIGHT + Fore.CYAN\nMAGENTA = Style.BRIGHT + Fore.MAGENTA\nRESET = Style.RESET_ALL\n\n# Data Frame and Plotly colors.\nFONT_COLOR = \"#8c564b\"\nBACKGROUND_COLOR = \"#FFFCFA\"\nGRADIENT_COLOR = \"#17becf\"\nCOLOR_SCHEME = np.array((\"#8c564b\", \"#FFFCFA\", \"#17becf\"))\nTICKSIZE = 11\n\n# Set Plotly theme.\npio.templates[\"minimalist\"] = go.layout.Template(\n    layout=go.Layout(\n        font_family=\"Open Sans\",\n        font_color=FONT_COLOR,\n        title_font_size=20,\n        plot_bgcolor=BACKGROUND_COLOR,\n        paper_bgcolor=BACKGROUND_COLOR,\n        xaxis=dict(tickfont_size=TICKSIZE, titlefont_size=TICKSIZE, showgrid=False),\n        yaxis=dict(tickfont_size=TICKSIZE, titlefont_size=TICKSIZE, showgrid=False),\n        width=840,\n        height=540,\n        legend=dict(yanchor=\"bottom\", xanchor=\"right\", orientation=\"h\", title=\"\"),\n    ),\n    layout_colorway=COLOR_SCHEME,\n)\npio.templates.default = \"plotly+minimalist\"\n\nMATPLOTLIB_THEME = {\n    \"axes.labelcolor\": FONT_COLOR,\n    \"axes.labelsize\": TICKSIZE,\n    \"axes.facecolor\": BACKGROUND_COLOR,\n    \"axes.titlesize\": 14,\n    \"axes.grid\": False,\n    \"xtick.labelsize\": TICKSIZE,\n    \"xtick.color\": FONT_COLOR,\n    \"ytick.labelsize\": TICKSIZE,\n    \"ytick.color\": FONT_COLOR,\n    \"figure.facecolor\": BACKGROUND_COLOR,\n    \"figure.edgecolor\": BACKGROUND_COLOR,\n    \"figure.titlesize\": 14,\n    \"figure.dpi\": 72,  # Locally Seaborn uses 72, meanwhile Kaggle 96.\n    \"text.color\": FONT_COLOR,\n    \"font.size\": TICKSIZE,\n    \"font.family\": \"Serif\",\n}\nsns.set_theme(rc=MATPLOTLIB_THEME)\n\n# Define Data Frame theme.\nCELL_HOVER = {  # for row hover use <tr> instead of <td>\n    \"selector\": \"td:hover\",\n    \"props\": f\"background-color: {BACKGROUND_COLOR}\",\n}\nTEXT_HIGHLIGHT = {\n    \"selector\": \"td\",\n    \"props\": f\"color: {FONT_COLOR}; font-weight: bold\",\n}\nINDEX_NAMES = {\n    \"selector\": \".index_name\",\n    \"props\": f\"font-weight: normal; background-color: {BACKGROUND_COLOR}; color: {FONT_COLOR};\",\n}\nHEADERS = {\n    \"selector\": \"th:not(.index_name)\",\n    \"props\": f\"font-weight: normal; background-color: {BACKGROUND_COLOR}; color: {FONT_COLOR};\",\n}\nDF_STYLE = (INDEX_NAMES, HEADERS, TEXT_HIGHLIGHT)\nDF_CMAP = sns.light_palette(GRADIENT_COLOR, as_cmap=True)\n\n# Html style for table of contents, code highlight and url.\nHTML_STYLE = \"\"\"\n    <style>\n    code {\n        background: rgba(42, 53, 125, 0.10) !important;\n        border-radius: 4px !important;\n    }\n    a {\n        color: rgba(123, 171, 237, 1.0) !important;\n    }\n    ol.numbered-list {\n        counter-reset: item;\n    }\n    ol.numbered-list li {\n        display: block;\n    }\n    ol.numbered-list li:before {\n        content: counters(item, '.') '. ';\n        counter-increment: item;\n    }\n    </style>\n\"\"\"\n\n\n# Utility functions.\ndef download_from_kaggle(expr, /, data_dir=None):\n    \"\"\"Download all files from the Kaggle competition/dataset.\n\n    Args:\n        expr: Match expression to be used by kaggle API, e.g.\n            \"kaggle competitions download -c competition\" or\n            \"kaggle datasets download -d user/dataset\".\n        data_dir: Optional. Directory path where to save files. Default to `None`,\n        which means that files will be downloaded to `data` directory.\n\n    Notes:\n        If the associated files already exists, then it does nothing.\n    \"\"\"\n\n    if data_dir is None:\n        data_dir = Path(\"data/\")\n    else:\n        data_dir = Path(data_dir)\n\n    match expr.split():\n        case [\"kaggle\", _, \"download\", *args] if args:\n            data_dir.mkdir(parents=True, exist_ok=True)\n            filename = args[-1].split(\"/\")[-1] + \".zip\"\n            if not (data_dir / filename).is_file():\n                subprocess.run(expr)\n                shutil.unpack_archive(filename, data_dir)\n                shutil.move(filename, data_dir)\n        case _:\n            raise SyntaxError(\"Invalid expression!\")\n\n\ndef get_interpolated_colors(color1, color2, /, n_colors=1):\n    \"\"\"Return `n_colors` colors in HEX format, interpolated beetwen `color1` and `color2`.\n\n    Args:\n        color1: Initial HEX color to be interpolated from.\n        color2: Final HEX color to be interpolated from.\n        n_colors: Optional. Number of colors to be interpolated between `color1`\n            and `color2`. Default to 1.\n\n    Returns:\n        colors: List of colors interpolated between `color1` and `color2`.\n    \"\"\"\n\n    def interpolate(color1, color2, t):\n        r1, g1, b1 = int(color1[1:3], 16), int(color1[3:5], 16), int(color1[5:7], 16)\n        r2, g2, b2 = int(color2[1:3], 16), int(color2[3:5], 16), int(color2[5:7], 16)\n        r = int(r1 + (r2 - r1) * t)\n        g = int(g1 + (g2 - g1) * t)\n        b = int(b1 + (b2 - b1) * t)\n        return f\"#{r:02X}{g:02X}{b:02X}\"\n\n    return [interpolate(color1, color2, k / (n_colors + 1)) for k in range(1, n_colors + 1)]\n\n\ndef get_pretty_frame(frame, /, gradient=False, formatter=None, precision=3, repr_html=False):\n    stylish_frame = frame.style.set_table_styles(DF_STYLE).format(\n        formatter=formatter, precision=precision\n    )\n    if gradient:\n        stylish_frame = stylish_frame.background_gradient(DF_CMAP)  # type: ignore\n    if repr_html:\n        stylish_frame = stylish_frame.set_table_attributes(\"style='display:inline'\")._repr_html_()\n    return stylish_frame\n\n\ndef numeric_descr(frame, /):\n    return (\n        frame.describe(percentiles=(0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99))\n        .T.drop(\"count\", axis=1)\n        .rename(columns=str.title)\n    )\n\n\ndef frame_summary(frame, /):\n    missing_vals = frame.isna().sum()\n    missing_vals_ratio = missing_vals / len(frame)\n    unique_vals = frame.apply(lambda col: len(col.unique()))\n    most_freq_count = frame.apply(lambda col: col.value_counts().iloc[0])\n    most_freq_val = frame.mode().iloc[:1].T.squeeze()\n    unique_ratio = unique_vals / len(frame)\n    freq_count_ratio = most_freq_count / len(frame)\n\n    return pd.DataFrame(\n        {\n            \"Dtype\": frame.dtypes,\n            \"MissingValues\": missing_vals,\n            \"MissingValuesRatio\": missing_vals_ratio,\n            \"UniqueValues\": unique_vals,\n            \"UniqueValuesRatio\": unique_ratio,\n            \"MostFreqValue\": most_freq_val,\n            \"MostFreqValueCount\": most_freq_count,\n            \"MostFreqValueCountRatio\": freq_count_ratio,\n        }\n    )\n\n\ndef check_categories_alignment(frame1, frame2, /, out_color=BLUE):\n    print(CLR + \"The same categories in training and test datasets?\\n\")\n    cat_features = frame2.select_dtypes(include=\"object\").columns.to_list()\n\n    for feature in cat_features:\n        frame1_unique = set(frame1[feature].unique())\n        frame2_unique = set(frame2[feature].unique())\n        same = np.all(frame1_unique == frame2_unique)\n        print(CLR + f\"{feature:25s}\", out_color + f\"{same}\")\n\n\ndef get_lower_triangular_frame(frame, /):\n    if not frame.shape[0] == frame.shape[1]:\n        raise ValueError(f\"{type(frame)!r} is not square frame\")\n    lower_triu = np.triu(np.ones_like(frame, dtype=bool))\n    frame = frame.mask(lower_triu)\n    return frame.dropna(axis=\"index\", how=\"all\").dropna(axis=\"columns\", how=\"all\")\n\n\ndef save_and_show_fig(fig, filename, /, img_dir=None, format=\"png\"):\n    if img_dir is None:\n        img_dir = Path(\"images\")\n    if not isinstance(img_dir, Path):\n        raise TypeError(\"The `img_dir` argument must be `Path` instance!\")\n\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / (filename + \".\" + format)\n    fig.write_image(fig_path)\n\n    return Image(fig.to_image(format=format))\n\n\ndef get_n_rows_and_axes(n_features, n_cols, /, start_at=1):\n    n_rows = int(np.ceil(n_features / n_cols))\n    current_col = range(start_at, n_cols + start_at)\n    current_row = range(start_at, n_rows + start_at)\n    return n_rows, tuple(product(current_row, current_col))\n\n\ndef get_kde_estimation(\n    series,\n    *,\n    bw_method=None,\n    weights=None,\n    percentile_range=(0, 100),\n    estimate_points_frac=0.1,\n    space_extension_frac=0.01,\n    cumulative=False,\n):\n    \"\"\"Return pdf dictionary for set of points using gaussian kernel density estimation.\n\n    Args:\n        series: The dataset with which `stats.gaussian_kde` is initialized.\n        bw_method: Optional. The method used to calculate the estimator bandwidth.\n        This can be 'scott', 'silverman', a scalar constant or a callable. If a scalar,\n        this will be used directly as `kde.factor`. If a callable, it should take\n        a `stats.gaussian_kde` instance as only parameter and return a scalar.\n        If `None` (default), 'scott' is used.\n        weights: Optional. Weights of datapoints. This must be the same shape as dataset.\n        If `None` (default), the samples are assumed to be equally weighted.\n        percentile_range: Optional. Percentile range of the `series` to create estimated space.\n        By default (0, 100) range is used.\n        estimate_points_frac: Optional. Fraction of `series` length to create linspace for\n        estimated points.\n        space_extension_frac: Optional. Estimation space will be extended by\n        `space_extension_frac * len(series)` for both edges.\n        cumulative: Optional. Whether to calculate cdf. Default to `False`.\n\n    Returns:\n        Dictionary with kde space, values, and cumulative values if `cumulative` is `True`.\n    \"\"\"\n\n    series = pd.Series(series).dropna()\n    kde = stats.gaussian_kde(series, bw_method=bw_method, weights=weights)\n    start, stop = np.percentile(series, percentile_range)\n\n    n_points = int(estimate_points_frac * len(series))\n    n_extend = int(space_extension_frac * len(series))\n\n    if n_extend > 0:\n        dx = (stop - start) / (n_points - 1)\n        start, stop = start - n_extend * dx, stop + n_extend * dx\n\n    kde_space = np.linspace(start, stop, n_points)\n    kde_vals = kde.evaluate(kde_space)\n    results = {\"space\": kde_space, \"vals\": kde_vals}\n\n    if cumulative:\n        kde_vals_cum = np.cumsum(kde_vals)\n        return results | {\"vals_cumulative\": kde_vals_cum / kde_vals_cum.max()}\n\n    return results\n\n\ndef unit_norm(x):\n    return x / np.sum(x)\n\n\n# Html highlight. Must be included at the end of all imports!\nHTML(HTML_STYLE)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-24T18:19:02.253506Z","iopub.execute_input":"2023-12-24T18:19:02.253896Z","iopub.status.idle":"2023-12-24T18:19:55.103118Z","shell.execute_reply.started":"2023-12-24T18:19:02.253864Z","shell.execute_reply":"2023-12-24T18:19:55.102122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">1.1</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Data Reading &amp; Features Description</span></b><a class=\"anchor\" id=\"data_reading_and_features_description\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"code","source":"competition = \"playground-series-s3e25\"\nexpr = f\"kaggle competitions download -c {competition}\"\n\nif not ON_KAGGLE:\n    download_from_kaggle(expr)\n    train_path = \"data/train.csv\"\n    test_path = \"data/test.csv\"\nelse:\n    train_path = f\"/kaggle/input/{competition}/train.csv\"\n    test_path = f\"/kaggle/input/{competition}/test.csv\"\n\ntrain = pd.read_csv(train_path, index_col=\"id\")  # .rename(columns=str.title)\ntest = pd.read_csv(test_path, index_col=\"id\")  # .rename(columns=str.title)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:55.105158Z","iopub.execute_input":"2023-12-24T18:19:55.105897Z","iopub.status.idle":"2023-12-24T18:19:55.266719Z","shell.execute_reply.started":"2023-12-24T18:19:55.105861Z","shell.execute_reply":"2023-12-24T18:19:55.265941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_pretty_frame(train.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:55.267853Z","iopub.execute_input":"2023-12-24T18:19:55.268176Z","iopub.status.idle":"2023-12-24T18:19:56.081881Z","shell.execute_reply.started":"2023-12-24T18:19:55.268152Z","shell.execute_reply":"2023-12-24T18:19:56.080874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info(verbose=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:56.082985Z","iopub.execute_input":"2023-12-24T18:19:56.083230Z","iopub.status.idle":"2023-12-24T18:19:56.096660Z","shell.execute_reply.started":"2023-12-24T18:19:56.083209Z","shell.execute_reply":"2023-12-24T18:19:56.095796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(verbose=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:56.099567Z","iopub.execute_input":"2023-12-24T18:19:56.099841Z","iopub.status.idle":"2023-12-24T18:19:56.110927Z","shell.execute_reply.started":"2023-12-24T18:19:56.099811Z","shell.execute_reply":"2023-12-24T18:19:56.110006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">1.2</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Basic Numerical Properties &amp; Summaries</span></b><a class=\"anchor\" id=\"basic_numerical_properties_summaries\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(\n    train,\n    x=\"Hardness\",\n    histnorm=\"probability\",\n    marginal=\"box\",\n    height=460,\n    title=\"Distribution of Hardness - Target Variable\"\n       )\nfig.update_yaxes(title=\"Probability\", row=1)\nsave_and_show_fig(fig, \"hardness_distribution\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:19:56.112111Z","iopub.execute_input":"2023-12-24T18:19:56.112422Z","iopub.status.idle":"2023-12-24T18:19:58.662805Z","shell.execute_reply.started":"2023-12-24T18:19:56.112399Z","shell.execute_reply":"2023-12-24T18:19:58.661794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CLR + \"Training Dataset:\")\ntrain_summary = frame_summary(train)\nget_pretty_frame(train_summary, gradient=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:58.664067Z","iopub.execute_input":"2023-12-24T18:19:58.664363Z","iopub.status.idle":"2023-12-24T18:19:58.710351Z","shell.execute_reply.started":"2023-12-24T18:19:58.664338Z","shell.execute_reply":"2023-12-24T18:19:58.709507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CLR + \"Test Dataset:\")\ntest_summary = frame_summary(test)\nget_pretty_frame(test_summary, gradient=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:58.711960Z","iopub.execute_input":"2023-12-24T18:19:58.712572Z","iopub.status.idle":"2023-12-24T18:19:58.750867Z","shell.execute_reply.started":"2023-12-24T18:19:58.712531Z","shell.execute_reply":"2023-12-24T18:19:58.749967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n  Missing  Unique Most Frequent Values\n\n   * What's surprising there is no missing values, so we don't need to bother about imputation. We can see the ratio of unique values in both datasets is quite low for all features, which means that features consist of many repeatable values. This makes them more semi-continuous rather than continuous. The target variable has only $50$ unique values among more than $10000$ training samples.\n","metadata":{}},{"cell_type":"code","source":"print(CLR + \"Training Dataset:\")\ntrain_num_descr = numeric_descr(train)\nget_pretty_frame(train_num_descr, gradient=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:58.752141Z","iopub.execute_input":"2023-12-24T18:19:58.752562Z","iopub.status.idle":"2023-12-24T18:19:58.810020Z","shell.execute_reply.started":"2023-12-24T18:19:58.752521Z","shell.execute_reply":"2023-12-24T18:19:58.809207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CLR + \"Test Dataset:\")\ntest_num_descr = numeric_descr(test)\nget_pretty_frame(test_num_descr, gradient=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:58.811168Z","iopub.execute_input":"2023-12-24T18:19:58.811497Z","iopub.status.idle":"2023-12-24T18:19:58.861669Z","shell.execute_reply.started":"2023-12-24T18:19:58.811467Z","shell.execute_reply":"2023-12-24T18:19:58.860798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adversarial Validation\nLet's test the last consideration with adversarial validation. What is the adversarial validation? Well, it's a very straightforward way to check whether our subsets are similar (sampled from the same or very similar distributions).We label training and test datasets with, for example, $0$ and $1$. Then, we combine them into one dataset and shuffle them. Subsequently, we can perform binary classification and assess if we're able to identify which observation is from which dataset. When we get a ROC value of around $0.5$ (random guessing), they are indistinguishable, and this case is desired. On the other hand, when ROC $>0.5$, it probably means that training and test subsets are from different distributions.\n","metadata":{}},{"cell_type":"code","source":"train_av = train.drop(\"Hardness\", axis=1).assign(AV=0)\ntest_av = test.assign(AV=1)\n\ndata_av = pd.concat((train_av, test_av), ignore_index=True)\ndata_av = data_av.sample(frac=1.0, random_state=42)\n\nX = data_av.drop(\"AV\", axis=1)\ny = data_av.AV\n\ny_proba = cross_val_predict(\n    estimator=make_pipeline(StandardScaler(), LogisticRegression(random_state=42)),\n    X=X,\n    y=y,\n    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=19937),\n    method=\"predict_proba\",\n)\n\nav_scores = {\n    \"ConfusionMatrix\": confusion_matrix(y, y_proba.argmax(axis=1)),\n    \"FPR-TPR-Threshold\": roc_curve(y, y_proba[:, 1]),\n    \"ROC-AUC\": roc_auc_score(y, y_proba[:, 1]),\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:19:58.863110Z","iopub.execute_input":"2023-12-24T18:19:58.863793Z","iopub.status.idle":"2023-12-24T18:19:59.256001Z","shell.execute_reply.started":"2023-12-24T18:19:58.863756Z","shell.execute_reply":"2023-12-24T18:19:59.254707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_scatter(\n    x=av_scores[\"FPR-TPR-Threshold\"][0],\n    y=av_scores[\"FPR-TPR-Threshold\"][1],\n    name=\"AV Result\",\n    mode=\"lines\",\n    line_color=COLOR_SCHEME[2],\n)\nfig.add_scatter(\n    x=[0, 1],\n    y=[0, 1],\n    name=\"Random Guess\",\n    mode=\"lines\",\n    line=dict(dash=\"longdash\", color=COLOR_SCHEME[0]),\n)\n\nfig.update_yaxes(\n    scaleanchor=\"x\",\n    scaleratio=1,\n    range=(-0.01, 1.01),\n    title=\"True Positive Rate (Recall)\",\n)\nfig.update_xaxes(\n    scaleanchor=\"y\",\n    scaleratio=1,\n    range=(-0.01, 1.01),\n    title=\"False Positive Rate (Fall-Out)\",\n)\nfig.update_layout(\n    title=\"Adversarial Validation Results\",    \n    width=540,\n    legend=dict(y=1.0, x=1.2),\n)\nsave_and_show_fig(fig, \"adversarial_validation\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:22:48.918539Z","iopub.execute_input":"2023-12-24T18:22:48.919471Z","iopub.status.idle":"2023-12-24T18:22:49.376449Z","shell.execute_reply.started":"2023-12-24T18:22:48.919432Z","shell.execute_reply":"2023-12-24T18:22:49.375338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adversarial Validation Results\n* So, the result is excellent for us since ROC $\\approx 0.5$ means that subsets are indistinguishable (they come from the same distribution).\n","metadata":{}},{"cell_type":"code","source":"features = test.columns.to_list()\n\nn_cols = 3\nn_rows, axes = get_n_rows_and_axes(len(features), n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    y_title=\"Probability Density\",\n    horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n).update_annotations(font_size=14)\n\nfor frame, color, group in zip((train, test), (COLOR_SCHEME[0], COLOR_SCHEME[2]), (\"Train\", \"Test\")):\n    for k, (var, (row, col)) in enumerate(zip(features, axes), start=1):\n        start, end = np.percentile(frame[var], (1, 99))\n        fig.add_histogram(\n            x=frame[var],\n            xbins=go.histogram.XBins(start=start, end=end),\n            histnorm=\"probability density\",\n            marker_color=color,\n            marker_line_width=0,\n            opacity=0.8,\n            name=group,\n            legendgroup=group,\n            showlegend=k == 1,\n            row=row,\n            col=col,\n        )\n        fig.update_xaxes(title_text=f\"{var}\", row=row, col=col)\n\nfig.update_layout(\n    width=840,\n    height=740,\n    legend=dict(y=1, x=1),\n    title=\"Training & Test Feature Histograms\",\n    bargap=0,\n    bargroupgap=0,\n)\nsave_and_show_fig(fig, \"histograms\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:22:55.736110Z","iopub.execute_input":"2023-12-24T18:22:55.736469Z","iopub.status.idle":"2023-12-24T18:22:58.643036Z","shell.execute_reply.started":"2023-12-24T18:22:55.736441Z","shell.execute_reply":"2023-12-24T18:22:58.642223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the features also indistinguisable between train and test set.","metadata":{}},{"cell_type":"code","source":"n_cols = 3\nn_rows, axes = get_n_rows_and_axes(len(features), n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    y_title=\"Probability Density\",\n    horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n).update_annotations(font_size=14)\n\nfor frame, color, group in zip((train, test), (COLOR_SCHEME[0], COLOR_SCHEME[2]), (\"Train\", \"Test\")):\n    for k, (var, (row, col)) in enumerate(zip(features, axes), start=1):\n        kde = get_kde_estimation(frame[var], percentile_range=(1, 99))\n        fig.add_scatter(\n            x=kde[\"space\"],\n            y=kde[\"vals\"],\n            line=dict(dash=\"solid\", color=color, width=1),\n            fill=\"tozeroy\",\n            name=group,\n            legendgroup=group,\n            showlegend=k == 1,\n            row=row,\n            col=col,\n        )\n        fig.update_xaxes(title_text=f\"{var}\", row=row, col=col)\n\nfig.update_layout(\n    width=840,\n    height=740,\n    legend=dict(y=1, x=1),\n    title=\"Training & Test Feature KDEs\"\n    ,\n)\nsave_and_show_fig(fig, \"kdes\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:23:03.782257Z","iopub.execute_input":"2023-12-24T18:23:03.782608Z","iopub.status.idle":"2023-12-24T18:23:07.839866Z","shell.execute_reply.started":"2023-12-24T18:23:03.782580Z","shell.execute_reply":"2023-12-24T18:23:07.838870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Feature Distributions</b> ðŸ“œ\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Feature distributions confirm the previous statement, i.e. training and test subsets probably follow the same distribution, which is reflected as bins overlapping and similar density estimation. Let's continue analysis with correlation matrix and hierarchical clustering. \n</p>","metadata":{}},{"cell_type":"code","source":"pearson_corr = train.corr(method=\"pearson\")\nlower_triu_corr = get_lower_triangular_frame(pearson_corr)\ncolormap = tuple(zip((0, 0.5, 1), COLOR_SCHEME[[1, 0, 2]]))\n\nheatmap = go.Heatmap(\n    z=lower_triu_corr,\n    x=lower_triu_corr.columns,\n    y=lower_triu_corr.index,\n    text=lower_triu_corr.fillna(\"\"),\n    texttemplate=\"%{text:.2f}\",\n    xgap=4,\n    ygap=4,\n    showscale=True,\n    colorscale=colormap,\n    colorbar_len=1.02,\n    hoverinfo=\"none\",\n)\nfig = go.Figure(heatmap)\nfig.update_layout(\n    title=\"Training Dataset - Lower Triangle of Correlation Matrix (Pearson)\",\n    yaxis_autorange=\"reversed\",\n    width=840,\n    height=840,\n)\nsave_and_show_fig(fig, \"pearson_corr_matrix\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:23:13.940201Z","iopub.execute_input":"2023-12-24T18:23:13.941029Z","iopub.status.idle":"2023-12-24T18:23:14.527745Z","shell.execute_reply.started":"2023-12-24T18:23:13.940978Z","shell.execute_reply":"2023-12-24T18:23:14.526586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abs_corr = (\n    lower_triu_corr.abs()\n    .unstack()\n    .sort_values(ascending=False)  # type: ignore\n    .rename(\"Absolute Pearson Correlation\")\n    .to_frame()\n    .reset_index(names=[\"Feature 1\", \"Feature 2\"])\n    .dropna()\n    .round(5)\n)\n\nwith pd.option_context(\"display.max_rows\", 10):\n    print(abs_corr)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:23:20.827315Z","iopub.execute_input":"2023-12-24T18:23:20.827976Z","iopub.status.idle":"2023-12-24T18:23:20.852449Z","shell.execute_reply.started":"2023-12-24T18:23:20.827940Z","shell.execute_reply":"2023-12-24T18:23:20.851470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dissimilarity = 1 - np.abs(pearson_corr)\n\nfig = ff.create_dendrogram(\n    dissimilarity,\n    labels=pearson_corr.columns,\n    orientation=\"left\",\n    colorscale=px.colors.sequential.Greys[3:],\n    # squareform() returns lower triangular in compressed form - as 1D array.\n    linkagefun=lambda x: linkage(squareform(dissimilarity), method=\"complete\"),\n)\nfig.update_xaxes(showline=False, title=\"Distance\", ticks=\"\", range=[-0.03, 1.05])\nfig.update_yaxes(showline=False, ticks=\"\")\nfig.update_layout(\n    title=\"Training Dataset - Hierarchical Clustering using Correlation Matrix (Pearson)<br>\",\n    height=460,\n    width=840,\n)\nfig.update_traces(line_width=1.5, opacity=1)\nsave_and_show_fig(fig, \"hierarchical_clustering\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:23:26.530746Z","iopub.execute_input":"2023-12-24T18:23:26.531116Z","iopub.status.idle":"2023-12-24T18:23:26.962895Z","shell.execute_reply.started":"2023-12-24T18:23:26.531085Z","shell.execute_reply":"2023-12-24T18:23:26.961877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_cols, n_features = 3, 6\nn_rows, axes = get_n_rows_and_axes(n_features, n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    horizontal_spacing=0.1,\n    vertical_spacing=0.15,\n)\n\nfor (row, col), (feature1, feature2, corr) in zip(axes, abs_corr[:n_features].to_numpy()):\n    fig.add_scatter(\n        x=train[feature1],\n        y=train[feature2],\n        mode=\"markers\",\n        name=\"\",\n        row=row,\n        col=col,\n    )\n    fig.update_xaxes(title_text=feature1, row=row, col=col)\n    fig.update_yaxes(title_text=feature2, row=row, col=col)\n\nfig.update_layout(\n    title=\"Training Dataset - Highly Linear Correlated Pairs\",\n    width=840,\n    height=540,\n    showlegend=False,\n)\nfig.update_traces(\n    marker=dict(size=1, symbol=\"x-thin\", line=dict(width=1.5, color=COLOR_SCHEME[0])),\n)\nsave_and_show_fig(fig, \"highly_correlated_scatter_plots\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:23:32.709863Z","iopub.execute_input":"2023-12-24T18:23:32.710310Z","iopub.status.idle":"2023-12-24T18:23:50.609520Z","shell.execute_reply.started":"2023-12-24T18:23:32.710277Z","shell.execute_reply":"2023-12-24T18:23:50.608622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Highly Linear Correlated Pairs\n\n* So, as we can see, there is a clearly visible high correlation between <code>allelectrons_Average</code> and <code>atomicweight_Average</code>. On the other hand, it's hard to say something about <code>ionenergy_Average</code> and <code>el_neg_chi_Average</code> because zero-points warp the correlation.\n","metadata":{}},{"cell_type":"code","source":"n_cols = 3\nn_rows, axes = get_n_rows_and_axes(len(features), n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    y_title=\"Hardness - Target Variable\",\n    horizontal_spacing=0.07,\n    vertical_spacing=0.1,\n)\nfig.update_annotations(font_size=14, yshift=-45)\n\nfor (row, col), feature in zip(axes, features):\n    fig.add_scatter(\n        x=train[feature],\n        y=train.Hardness,\n        mode=\"markers\",\n        name=feature,\n        row=row,\n        col=col,\n    )\n    fig.update_xaxes(\n        title_text=f\"<b>{feature}</b>\",\n        row=row,\n        col=col,\n    )\n    if not col == 1:\n        fig.update_yaxes(showticklabels=False, row=row, col=col)\n\nfig.update_layout(\n    title=\"Training Dataset - Hardness vs Remaining Features\",\n    width=840,\n    height=840,\n    showlegend=False,\n)\nfig.update_traces(\n    marker=dict(size=1, symbol=\"x-thin\", line=dict(width=1.5, color=COLOR_SCHEME[0])),\n)\nsave_and_show_fig(fig, \"scatter_plots\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:24:00.169217Z","iopub.execute_input":"2023-12-24T18:24:00.169580Z","iopub.status.idle":"2023-12-24T18:24:31.643551Z","shell.execute_reply.started":"2023-12-24T18:24:00.169550Z","shell.execute_reply":"2023-12-24T18:24:31.642600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">1.3</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Probability Plots &amp; Example Transformations</span></b><a class=\"anchor\" id=\"probability_plots_and_example_transformations\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    This subsection focuses on the exploration of probability plots, which are a graphical technique used to determine if a variable adheres to a particular distribution, specifically the normal distribution in this case. <b>Probability plots display samples that follow a normal distribution along a straight diagonal line.</b> Some machine learning models make the assumption that variables follow a normal distribution. Consequently, the mentioned technique assists in determining the necessary transformations to improve the variable's alignment with that distribution. We will begin with examining the original values and observing the outcomes.\n</p>","metadata":{}},{"cell_type":"code","source":"n_cols = 3\nn_rows, axes = get_n_rows_and_axes(len(features), n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    y_title=\"Observed Values\",\n    x_title=\"Theoretical Quantiles\",\n    horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n)\nfig.update_annotations(font_size=14, yshift=-45)\n\nfor (row, col), feature in zip(axes, features):\n    (osm, osr), (slope, intercept, R) = stats.probplot(train[feature].dropna(), rvalue=True)\n    x_theory = np.array([osm[0], osm[-1]])\n    y_theory = intercept + slope * x_theory\n    R2 = f\"R\\u00b2 = {R * R:.2f}\"\n    fig.add_scatter(x=osm, y=osr, mode=\"markers\", row=row, col=col, name=feature)\n    fig.add_scatter(x=x_theory, y=y_theory, mode=\"lines\", row=row, col=col)\n    fig.add_annotation(\n        x=-1.25,\n        y=osr[-1] * 0.95,\n        text=R2,\n        showarrow=False,\n        row=row,\n        col=col,\n        font_size=11,\n    )\n    fig.update_xaxes(\n        title_text=f\"{feature}\",\n        row=row,\n        col=col,\n    )\n\nfig.update_layout(\n    title=\"Training Dataset - Probability Plots against Normal Distribution\",\n    width=840,\n    height=840,\n    showlegend=False,\n)\nfig.update_traces(\n    marker=dict(size=1, symbol=\"x-thin\", line=dict(width=2, color=COLOR_SCHEME[2])),\n    line_color=COLOR_SCHEME[0],\n)\nsave_and_show_fig(fig, \"probability_plots\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:24:47.266388Z","iopub.execute_input":"2023-12-24T18:24:47.266965Z","iopub.status.idle":"2023-12-24T18:25:18.894343Z","shell.execute_reply.started":"2023-12-24T18:24:47.266933Z","shell.execute_reply":"2023-12-24T18:25:18.893424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probability Plots against Normal Distribution\n\n* Some variables fit a normal distribution well, which manifests by a high coefficient of determination (R-squared) and evenly deployed samples around the straight line. However, there are same of features which have a poor fit. We can improve that through specific transformations. Mostly used transformations are log-level and square-root ones. These work fine with right-skewed data and help to reduce the impact of outliers. Another transformation is, for example, a reciprocal one, which is sometimes used when data is skewed, or there are obvious outliers. More sophisticated methods are Box-Cox transformation (requires strictly positive numbers) and Yeo-Johnson (variation of Box-Cox), which has no restrictions concerning numbers. We will check three of mentioned: log-level, square-root and Yeo-Johnson. For this case, we will utilise the <code>probplot</code> function from the <code>scipy</code> module to get R-squared coefficients as earlier.\n","metadata":{}},{"cell_type":"code","source":"r2_scores = pd.DataFrame(index=(\"Original\", \"YeoJohnson\", \"Log\", \"Sqrt\"))\n\nfor feature in features:\n    orig = train[feature].dropna()\n    _, (*_, R_orig) = stats.probplot(orig, rvalue=True)\n    _, (*_, R_yeojohn) = stats.probplot(stats.yeojohnson(orig)[0], rvalue=True)\n    _, (*_, R_log) = stats.probplot(np.log1p(orig), rvalue=True)\n    _, (*_, R_sqrt) = stats.probplot(np.sqrt(orig), rvalue=True)\n\n    r2_scores[feature] = (\n        R_orig * R_orig,\n        R_yeojohn * R_yeojohn,\n        R_log * R_log,\n        R_sqrt * R_sqrt,\n    )\n\nr2_scores = r2_scores.transpose()\nr2_scores[\"Winner\"] = r2_scores.idxmax(axis=1)\nget_pretty_frame(r2_scores)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:25:36.647382Z","iopub.execute_input":"2023-12-24T18:25:36.647785Z","iopub.status.idle":"2023-12-24T18:25:36.826756Z","shell.execute_reply.started":"2023-12-24T18:25:36.647751Z","shell.execute_reply":"2023-12-24T18:25:36.825776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"R-squared Scores within Some Transformations\n    \n    Well, as you can see <b>Yeo-Johnson's transformation wins in all cases and improves fit to the normal distribution pretty well</b> ($R^2$ scores almost in all cases are above $0.90$). However, there is a one feature where none of the transformations helps, i.e. <code>zaratio_Average</code>. Probably it's the effect of specific shape of this feature (looks like a half-normal distribution). Let's see how Yeo-Johnson transformation helps with specific feature, for example, the <code>density_Total</code> one.\n</p>","metadata":{}},{"cell_type":"code","source":"density_Total_transformed = stats.yeojohnson(train.density_Total.dropna())[0]\n(osm, osr), (slope, intercept, R) = stats.probplot(density_Total_transformed, rvalue=True)\nx_theory = np.array([osm[0], osm[-1]])\ny_theory = intercept + slope * x_theory\n\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    subplot_titles=[\"Probability Plot against Normal Distribution\", \"Distribution\"],\n    horizontal_spacing=0.15,\n)\n\nfig.add_scatter(x=osm, y=osr, mode=\"markers\", row=1, col=1, name=\"YeoJohnson(density_Total)\")\nfig.add_scatter(x=x_theory, y=y_theory, mode=\"lines\", row=1, col=1)\nfig.add_annotation(\n    x=-1.25,\n    y=osr[-1] * 0.75,\n    text=f\"R\\u00b2 = {R * R:.3f}\",\n    showarrow=False,\n    row=1,\n    col=1,\n)\nfig.update_yaxes(title_text=\"Observed Values\", row=1, col=1)\nfig.update_xaxes(title_text=\"Theoretical Quantiles\", row=1, col=1)\nfig.update_traces(\n    marker=dict(size=1, symbol=\"x-thin\", line=dict(width=2, color=COLOR_SCHEME[2])),\n    line_color=COLOR_SCHEME[0],\n)\n\nfig.add_histogram(\n    x=density_Total_transformed,\n    xbins=go.histogram.XBins(size=0.1),\n    marker_color=COLOR_SCHEME[0],\n    name=\"YeoJohnson(density_Total)\",\n    histnorm=\"probability density\",\n    row=1,\n    col=2,\n)\nfig.update_yaxes(title_text=\"Probability Density\", row=1, col=2)\nfig.update_xaxes(title_text=\"YeoJohnson(density_Total)\", row=1, col=2)\n\nfig.update_layout(\n    title=\"Yeo-Johnson Transformation for 'density_Total' Feature\",\n    showlegend=False,\n    width=840,\n    height=460,\n    bargap=0.2,\n)\nfig.update_annotations(font_size=14)\nsave_and_show_fig(fig, \"density_Total_after_transform\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:25:42.951656Z","iopub.execute_input":"2023-12-24T18:25:42.952366Z","iopub.status.idle":"2023-12-24T18:25:48.550627Z","shell.execute_reply.started":"2023-12-24T18:25:42.952335Z","shell.execute_reply":"2023-12-24T18:25:48.549749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Features Importance</span></b><a class=\"anchor\" id=\"features_importance\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>About Section</b> ðŸ’¡\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    In this section, we will tackle the general problem of features' importance. Generally, sometimes not all variables are crucial during the training process, and only some are relevant for specific models. There are many methods, from selecting some top features based on the ANOVA test, mutual information, up to recursive feature selection with cross-validation. We can also select features from given models like random forest using the importance ratio. Generally, different methods may give different results. Moreover, it's good to include random variables in training data and measure their importance. <b>If some random numbers are more important than given features, it means that those variables are useless (may introduce a noise) from the problem perspective (but still can be useful in other tasks).</b> In this section, we will investigate decision process in a simple decision tree, and the we will focus on gradient boosting trees. We will see permutation tests, random variables, mutual information, one-way partial dependence plots (PDPs) and two-way PDPs.<br><br>\n    <b>This section has a showcase character and all what can you see here may be slightly different depending on used machine learning algorithm or random seeds. Nevertheless, it will show us, in general, which features are more important and which are not.</b>\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2.1</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Decision Process in Simple Decision Tree</span></b><a class=\"anchor\" id=\"simple_decision_tree_and_its_decision_process\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"code","source":"X = train.drop(\"Hardness\", axis=1)\ny = train.Hardness\n\nDefaultDecisionTreeRegressor = partial(\n    DecisionTreeRegressor,\n    criterion=\"absolute_error\",  # Watch out on learning time complexity.\n    random_state=42,\n    max_depth=3,\n)\n\ntree = DefaultDecisionTreeRegressor().fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:26:06.371915Z","iopub.execute_input":"2023-12-24T18:26:06.372943Z","iopub.status.idle":"2023-12-24T18:26:08.772567Z","shell.execute_reply.started":"2023-12-24T18:26:06.372905Z","shell.execute_reply":"2023-12-24T18:26:08.771719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(11.5, 5.5), tight_layout=True)\nplot_tree(\n    decision_tree=tree,\n    feature_names=tree.feature_names_in_.tolist(),\n    filled=False,\n    rounded=True,\n    impurity=False,\n    proportion=True,\n    node_ids=True,\n    ax=plt.gca(),\n    fontsize=11,\n)\nplt.title(\"Decision Process in Decision Tree (depth = 3)\")\nplt.savefig(\"images/decision_process_in_tree\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:26:13.314557Z","iopub.execute_input":"2023-12-24T18:26:13.315437Z","iopub.status.idle":"2023-12-24T18:26:15.322576Z","shell.execute_reply.started":"2023-12-24T18:26:13.315400Z","shell.execute_reply":"2023-12-24T18:26:15.321549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for depth in range(2, 7):\n    tree.set_params(max_depth=depth).fit(X, y)\n    considered_features = tree.tree_.feature[tree.tree_.feature != -2]  # type: ignore # -2 means a leaf\n    used_features = np.unique(considered_features)\n    used_features = X.columns[used_features].to_list()\n    print(CLR + f\"Features at depth {depth}: {RED}{len(used_features):<5}\", end=\"\")\n    tree_cv_results = -cross_val_score(\n        estimator=tree,\n        X=X,\n        y=y,\n        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n        scoring=\"neg_median_absolute_error\",\n        n_jobs=2,\n    )\n    mean, std = tree_cv_results.mean(), tree_cv_results.std()\n    print(CLR + \"MedAE:\", RED + f\"{mean:.2f} \\u00b1 {std:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:26:19.210115Z","iopub.execute_input":"2023-12-24T18:26:19.210489Z","iopub.status.idle":"2023-12-24T18:27:01.522040Z","shell.execute_reply.started":"2023-12-24T18:26:19.210457Z","shell.execute_reply":"2023-12-24T18:27:01.520911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Decision Process in Tree</b> ðŸ“œ\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    So, we've got here a simple, default decision tree within a depth of $3$. I used such a depth to easily show you nodes. Subsequently, I performed cross-validation to assess how depth influences the outcome, and it turns out that we get the same output as the depth is equal to $3$ or more. So, three features are enough to get MedAE $= 0.5$. Is that decent? Probably not. Moreover, note that number of considered features at depth $4$ increases from $3$ to $8$, compared to depth $3$. \n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2.2</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Random Variables &amp; Permutation Test</span></b><a class=\"anchor\" id=\"random_variables_permutation_test\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Here, we're going to start by introducing random variables to the dataset. Why? Well, when you introduce random variables to the dataset and train the model, you may check feature importances, for example, based on reduction in MAE criterion. <b>When a random variable contributes to the reduction of MAE more than the specific feature available in the dataset, it means that this feature is a noise indeed from the task perspective.</b>\n</p>","metadata":{}},{"cell_type":"code","source":"DefaultLGBMRegressor = partial(\n    LGBMRegressor,\n    objective=\"regression_l1\",\n    random_state=42,\n    verbose=-1,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:27:30.222762Z","iopub.execute_input":"2023-12-24T18:27:30.223130Z","iopub.status.idle":"2023-12-24T18:27:30.227957Z","shell.execute_reply.started":"2023-12-24T18:27:30.223103Z","shell.execute_reply":"2023-12-24T18:27:30.227070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nseeds = np.random.randint(0, 19937, size=5)\n\nX = train.drop(\"Hardness\", axis=1)\ny = train.Hardness\n\nlgbm = DefaultLGBMRegressor()\nimportances = []\n\nfor seed in seeds:\n    np.random.seed(seed)\n    X[\"RANDOM_1\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_2\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_3\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_4\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_5\"] = np.random.normal(size=len(X))\n\n    lgbm.set_params(random_state=seed).fit(X, y)\n    importances.append(unit_norm(lgbm.feature_importances_))\n\nimportances = (\n    pd.DataFrame({\"Feature\": X.columns, \"Importance\": np.array(importances).mean(axis=0)})\n    .sort_values(by=\"Importance\", ascending=False)\n    .reset_index(drop=True)\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:27:38.111479Z","iopub.execute_input":"2023-12-24T18:27:38.112217Z","iopub.status.idle":"2023-12-24T18:27:39.392535Z","shell.execute_reply.started":"2023-12-24T18:27:38.112184Z","shell.execute_reply":"2023-12-24T18:27:39.391551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(\n    importances,\n    x=\"Importance\",\n    y=\"Feature\",\n    height=460,\n    width=840,\n    title=\"Feature Importances in LGBM Regressor - Under Reduction in MAE Criterion<br>\",\n)\nfig.update_yaxes(categoryorder=\"total ascending\", title=\"\")\nfig.update_xaxes(range=(-0.002, 0.11))\nfig.update_traces(width=0.7)\nsave_and_show_fig(fig, \"importance_with_mae_reduction\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:27:48.420717Z","iopub.execute_input":"2023-12-24T18:27:48.421359Z","iopub.status.idle":"2023-12-24T18:27:48.759600Z","shell.execute_reply.started":"2023-12-24T18:27:48.421325Z","shell.execute_reply":"2023-12-24T18:27:48.758679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Feature Importances via Reduction in MAE</b> ðŸ“œ\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Here I used $5$ different seeds to gain more reliable results. <b>As we can see, in this specific situation, most of variables are more important than random ones.</b> However, some of them are at the randomness level. What is more, if we had defined only one random feature, it could turn out that we've been just lucky that it's essential or not. We have a more general recognition when we define several of them.\n</p>","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nseeds = np.random.randint(0, 19937, size=5)\n\nlgbm = DefaultLGBMRegressor()\npermutation_medae = defaultdict(list)\n\nfor seed in seeds:\n    np.random.seed(seed)\n    X[\"RANDOM_1\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_2\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_3\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_4\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_5\"] = np.random.normal(size=len(X))\n\n    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n    lgbm.set_params(random_state=seed)\n\n    for k, (train_ids, valid_ids) in enumerate(kfold.split(X, y)):\n        X_train, y_train = X.iloc[train_ids], y[train_ids]  # type: ignore\n        X_valid, y_valid = X.iloc[valid_ids], y[valid_ids]  # type: ignore\n\n        lgbm.fit(X_train, y_train)\n        medae = median_absolute_error(y_valid, lgbm.predict(X_valid))  # type: ignore\n\n        for i, feature in enumerate(X_train.columns):\n            X_shuffled = X_valid.copy()\n            X_shuffled.iloc[:, i] = np.random.permutation(X_shuffled.iloc[:, i])\n            medae_shuffled = median_absolute_error(y_valid, lgbm.predict(X_shuffled))  # type: ignore\n            # I assume an increase in MedAE if the attribute is essential.\n            permutation_medae[feature].append(((medae_shuffled - medae) / medae) * 100.0)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:27:54.234201Z","iopub.execute_input":"2023-12-24T18:27:54.235011Z","iopub.status.idle":"2023-12-24T18:28:04.240183Z","shell.execute_reply.started":"2023-12-24T18:27:54.234980Z","shell.execute_reply":"2023-12-24T18:28:04.239192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medae_increase = (\n    pd.DataFrame(permutation_medae)\n    .mean()\n    .sort_values(ascending=False)\n    .to_frame(name=\"Mean MedAE Increase (%)\")\n    .reset_index(names=\"Feature\")\n)\n\nfig = px.bar(\n    medae_increase,\n    x=\"Mean MedAE Increase (%)\",\n    y=\"Feature\",\n    height=460,\n    width=840,\n    title=\"Mean MedAE Increase in LGBM Regressor within Samples Permutation<br>\",\n)\nfig.update_yaxes(categoryorder=\"total ascending\", title=\"\")\nfig.update_xaxes(range=(-1, 55))\nfig.update_traces(width=0.7)\nsave_and_show_fig(fig, \"importance_with_feature_permutation\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:28:16.022008Z","iopub.execute_input":"2023-12-24T18:28:16.022918Z","iopub.status.idle":"2023-12-24T18:28:16.315721Z","shell.execute_reply.started":"2023-12-24T18:28:16.022884Z","shell.execute_reply":"2023-12-24T18:28:16.314793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Samples Permutation Test</b> ðŸ“œ\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    In the code above, we explore how rearranging samples within a specific feature affects MedAE when evaluating the validation dataset. To ensure more reliable outcomes, this entire process is repeated with different random seeds. Importantly, throughout this entire process, we shuffle samples in the chosen feature of the validation subset and record results obtained from evaluating this modified dataset in a separate dictionary. If the variable is significant, we should observe worsened results in terms of MedAE. <b>As we can see, some features punish the model mostly. But it doesn't concern random variables.</b> For <code>density_Total</code>, <code>density_Average</code>, and <code>allelectrons_Total</code> variables the situation is similar to random features.<br><br>\n    Everything I show here is only illustrative, and the aim is to gain a little intuition about available features. Different models can recognise various features as relevant ones. Moreover, when you set a certain depth or minimum number of samples in a leaf in the tested gradient boosting regressor, you probably get a slightly different outcome. \n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2.3</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Mutual Information</span></b><a class=\"anchor\" id=\"mutual_information\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Let's check feature importance via the mutual information method. Generally, mutual information is a quantity that measures the relation between simultaneously sampled variables. In other words, it measures how much information about one variable is, on average, enclosed in the second variable. Intuitively, we can ask how much one variable tells us about the second one. <b>The theorem says that the mutual information between two variables is zero if and only if these are statistically independent.</b>\n</p>","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nseeds = np.random.randint(0, 19937, size=5)\n\nscaler = StandardScaler()\nmutual_info = []\n\nfor seed in seeds:\n    np.random.seed(seed)\n    X[\"RANDOM_1\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_2\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_3\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_4\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_5\"] = np.random.normal(size=len(X))\n\n    # Choose of neighbors is subjective.\n    mi = mutual_info_regression(X=scaler.fit_transform(X), y=y, n_neighbors=50, random_state=seed)\n    mutual_info.append(mi)\n\nmi_importances = (\n    pd.DataFrame({\"Feature\": X.columns, \"Mutual Information\": np.array(mutual_info).mean(axis=0)})\n    .sort_values(by=\"Mutual Information\", ascending=False)\n    .reset_index(drop=True)\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:28:22.761853Z","iopub.execute_input":"2023-12-24T18:28:22.762213Z","iopub.status.idle":"2023-12-24T18:28:34.054975Z","shell.execute_reply.started":"2023-12-24T18:28:22.762184Z","shell.execute_reply":"2023-12-24T18:28:34.053977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(\n    mi_importances,\n    x=\"Mutual Information\",\n    y=\"Feature\",\n    height=460,\n    width=840,\n    title=\"Feature Importances via Mutual Information\",\n)\nfig.update_yaxes(categoryorder=\"total ascending\", title=\"\")\nfig.update_xaxes(range=(-0.005, 0.3))\nfig.update_traces(width=0.7)\nsave_and_show_fig(fig, \"mutual_information\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:28:45.631255Z","iopub.execute_input":"2023-12-24T18:28:45.632171Z","iopub.status.idle":"2023-12-24T18:28:45.933975Z","shell.execute_reply.started":"2023-12-24T18:28:45.632127Z","shell.execute_reply":"2023-12-24T18:28:45.932956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Mutual Information Results</b> ðŸ“œ\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    As we can see, mutual information says that all features are more important than randome variables. Therefore, this is a slightly different outcome than in tests with random forest. Let's get to something more interesting, i.e. partial dependence plots.\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2.4</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Partial Dependence for Features of Interest</span></b><a class=\"anchor\" id=\"partial_dependence_for_features_of_interest\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Generally, a partial dependence plot (PDP) is another tool for visualizing feature importance. However, this approach differs slightly from the earlier depicted. <b>Here, the partial dependence plot shows the relationship between the model outcome and a particular feature or a set of particular features.</b> In this case, the outcome (partial dependence) is an output of the <code>predict()</code> method. So, it's just a prediction for the <code>Hardness</code> variable. In other words, according to <code>scikit-learn</code> docs: <i>Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.</i> Let's have a look at how it works and how it looks.\n</p>","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\nX = train.drop(\"Hardness\", axis=1).assign(RANDOM_1=np.random.normal(size=len(train)))\ny = train.Hardness\n\nlgbm = DefaultLGBMRegressor().fit(X, y)\n\nfig, axes = plt.subplots(4, 3, figsize=(11.5, 10), tight_layout=True, sharey=True)\nplt.suptitle(\"One-Variable Partial Dependence in LGBM Regressor\")\nPartialDependenceDisplay.from_estimator(\n    estimator=lgbm,  # type: ignore\n    X=X,\n    features=X.columns.tolist(),\n    feature_names=X.columns.tolist(),\n    response_method=\"auto\",  # In regression, the response is `predict()` output.\n    kind=\"both\",  # PDP and ICE.\n    percentiles=(0.01, 0.99),\n    subsample=0.5,\n    random_state=42,\n    n_jobs=-1,\n    ice_lines_kw={\"color\": COLOR_SCHEME[0], \"linewidth\": 0.2, \"alpha\": 0.1, \"linestyle\": \"--\"},\n    pd_line_kw={\"color\": COLOR_SCHEME[2], \"linewidth\": 2.0},\n    ax=axes.ravel(),  # type: ignore\n)\n\nfor ax in axes.ravel():\n    ax.get_legend().remove()\n    if ax not in (axes[0, 0], axes[1, 0], axes[2, 0], axes[3, 0]):\n        ax.set_ylabel(\"\")\n\nplt.savefig(\"images/one_way_partial_dependence\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:28:51.998057Z","iopub.execute_input":"2023-12-24T18:28:51.998864Z","iopub.status.idle":"2023-12-24T18:31:37.260402Z","shell.execute_reply.started":"2023-12-24T18:28:51.998828Z","shell.execute_reply":"2023-12-24T18:31:37.259475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Partial Dependence</b> ðŸ“œ\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Here we have PDP visualizations all features of interest. I included one random variable to show you how looks a feature that does not drive any influence.This way, we can see how different features impact the model, whether it's a linear dependence or not. What is more, one needs to add something. <b>Actually, we've created PDP and individual conditional expectation (ICE) plots. ICE is similar to PDP, but here, the ICE plot shows the dependency of the prediction within a given feature for each sample (it means each black line corresponds to a specific sample).</b><br><br>\n    Well, so what do we see here? I describe it using the <code>ionenergy_Average</code> feature of interest. Firstly, we need to remember that interactions between features are not included here. It means that we see predictions of the model (partial dependence) depending on only one feature. Concerning the <code>ionenergy_Average</code> variable, we can see that initially the output is constant as the feature grows, i.e. the dependence is constant in the range $(8, 9)$ of <code>ionenergy_Average</code>. On a higher cut-off, the <code>ionenergy_Average</code> starts to drive an influence on the model, i.e., in the range $~9.0$ the dependence is positive and (probably?) non-linear. Next, in the range $(9, 12.0)$ it's a constant again, and subsequently has a negative impact. <b>So there are narrow ranges of this feature where the outcome rapidly grows or drops.</b><br><br>\n    Now let's have a look at random variable. The dependence is always constant here, which means that this feature doesn't introduce any information. Now let's have a look at PDP with two input features of interest. That will show us interactions.\n</p>","metadata":{}},{"cell_type":"code","source":"interaction_pair1 = [\"allelectrons_Average\", \"atomicweight_Average\"]\ninteraction_pair2 = [\"zaratio_Average\", \"ionenergy_Average\"]\n\nX = train[np.union1d(interaction_pair1, interaction_pair2)]\ny = train.Hardness\nlgbm = DefaultLGBMRegressor().fit(X, y)  # type: ignore\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11.5, 5.5), tight_layout=True)\nplt.suptitle(\"Two-Variable PDP in LGBM Regressor\")\nPartialDependenceDisplay.from_estimator(\n    estimator=lgbm,  # type: ignore\n    X=X,\n    features=[interaction_pair1, interaction_pair2],\n    feature_names=X.columns.to_list(),\n    response_method=\"auto\",  # In regression, the response is `predict()` output.\n    percentiles=(0.01, 0.99),\n    random_state=42,\n    n_jobs=-1,\n    contour_kw={\"cmap\": \"pink\"},\n    ax=axes,  # type: ignore\n)\n\nplt.savefig(\"images/two_way_partial_dependence\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:32:58.595559Z","iopub.execute_input":"2023-12-24T18:32:58.596336Z","iopub.status.idle":"2023-12-24T18:41:30.042760Z","shell.execute_reply.started":"2023-12-24T18:32:58.596298Z","shell.execute_reply":"2023-12-24T18:41:30.041813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Interactions</b> ðŸ“œ\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    The two-variable PDP above shows the dependence of the target variable on joint values of two pairs, i.e. (<code>allelectrons_Average</code>, <code>atomicweight_Average</code>) and (<code>zaratio_Average</code>, <code>ionenergy_Average</code>). Here, for example, we can see that if the <code>zaratio_Average</code> is greater than $0.5$, the main impact on prediction has <code>ionenergy_Average</code>, but when <code>zaratio_Average</code> is in the range $(0.45, 0.50)$, the situation is diversed.\n</p>","metadata":{}},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">3</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Dimensionality Reduction</span></b><a class=\"anchor\" id=\"dimensionality_reduction\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    There we have several interesting dimensionality reduction algorithms, like t-SNE, PCA or LLE. For example the t-SNE method is a great reduction technique commonly used for visualizing complex and high-dimensional data in a lower-dimensional space. Moreover, it tries to preserve the original structure of data, so similar samples should be deployed close to each other in reduced dimensions. Using t-SNE we should provide scaled data, otherwise, certain dimensions can be dominated by features with larger scales or units. For the purpose of this notebook, we can test two techniques, i.e. PCA, Isomap and t-SNE.\n</p>","metadata":{}},{"cell_type":"code","source":"X = train.drop(\"Hardness\", axis=1)\ny = train.Hardness\n\ntransformer = PowerTransformer(method=\"yeo-johnson\", standardize=True)\nX_rescaled = transformer.fit_transform(X)\n\npca_2d = PCA(n_components=2, random_state=42)\niso_2d = Isomap(n_components=2, n_neighbors=20, n_jobs=-1)\ntsne_2d = TSNE(n_components=2, random_state=42, n_jobs=-1)\n\npca_2d_results = pd.DataFrame(pca_2d.fit_transform(X_rescaled), columns=(\"x1\", \"x2\")).join(y)\niso_2d_results = pd.DataFrame(iso_2d.fit_transform(X_rescaled), columns=(\"x1\", \"x2\")).join(y)\ntsne_2d_results = pd.DataFrame(tsne_2d.fit_transform(X_rescaled), columns=(\"x1\", \"x2\")).join(y)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:41:44.282224Z","iopub.execute_input":"2023-12-24T18:41:44.282575Z","iopub.status.idle":"2023-12-24T18:44:44.573881Z","shell.execute_reply.started":"2023-12-24T18:41:44.282550Z","shell.execute_reply":"2023-12-24T18:44:44.572787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_cols, n_projections = 3, 3\nn_rows, axes = get_n_rows_and_axes(n_projections, n_cols)\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    subplot_titles=(\"PCA\", \"Isomap\", \"TSNE\"),\n    x_title=\"x1\",\n    y_title=\"x2\",\n    # horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n)\n\nfor (row, col), projection in zip(axes, (pca_2d_results, iso_2d_results, tsne_2d_results)):\n    fig.add_scatter(\n        x=projection.x1,\n        y=projection.x2,\n        mode=\"markers\",\n        marker=dict(size=1, color=projection.Hardness, coloraxis=\"coloraxis\"),\n        row=row,\n        col=col,\n        showlegend=False,\n    )\n\nfig.update_annotations(font_size=14, yshift=-15)\nfig.update_coloraxes(\n    colorbar=dict(\n        title_text=\"Hardness\",\n        ticklabelposition=\"outside bottom\",\n        orientation=\"h\",\n        title_side=\"bottom\",\n        yanchor=\"bottom\",\n        xanchor=\"center\",\n        len=1.02,\n        y=-0.5,\n        x=0.5,\n    ),\n    colorscale=colormap,\n)\nfig.update_layout(\n    title=\"Training Dataset - Dimensionality Reduction with Different Algorithms\",\n    width=840,\n    height=440,\n)\nsave_and_show_fig(fig, \"projections_2d\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:44:52.109700Z","iopub.execute_input":"2023-12-24T18:44:52.110130Z","iopub.status.idle":"2023-12-24T18:45:00.657648Z","shell.execute_reply.started":"2023-12-24T18:44:52.110095Z","shell.execute_reply":"2023-12-24T18:45:00.656645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Dimensionality Reduction Results</b> ðŸ“œ\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    As we can see, PCA and Isomap produce kind similar outcomes. On the other hand, t-SNE creates a pretty cool low-dimensional visual. Let's get one step further and use t-SNE to create a 3D projection.\n</p>","metadata":{}},{"cell_type":"code","source":"tsne_3d = TSNE(n_components=3, random_state=42, n_jobs=-1)\ntsne_3d_results = pd.DataFrame(tsne_3d.fit_transform(X_rescaled), columns=(\"x1\", \"x2\", \"x3\")).join(y)\n\nfig = px.scatter_3d(\n    tsne_3d_results,\n    x=\"x1\",\n    y=\"x2\",\n    z=\"x3\",\n    color=\"Hardness\",\n    color_continuous_scale=colormap,\n    opacity=0.5,\n    height=840,\n    width=840,\n    title=\"Training Dataset - 3D Projection with t-SNE<br>\",\n)\nfig.update_traces(marker_size=2)\nfig.update_coloraxes(colorbar=dict(ticklabelposition=\"outside bottom\"))\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:45:05.810318Z","iopub.execute_input":"2023-12-24T18:45:05.811174Z","iopub.status.idle":"2023-12-24T18:48:51.619161Z","shell.execute_reply.started":"2023-12-24T18:45:05.811137Z","shell.execute_reply":"2023-12-24T18:48:51.618187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dimensionality Reduction with t-SNE\n\n\n    So, here we see that t-SNE tried to separate samples with different Hardness value levels. For example, there is an area that is dominated by samples with low Hardness. Well, dimensionality reduction is a nice tool but in this specific problem samples seems to be difficult to separate them clearly.\n</p>","metadata":{}},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">4</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Outliers Detection</span></b><a class=\"anchor\" id=\"outliers_detection\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"markdown","source":"\n    Generally, there are several methods available in the scikit-learnlibrary if you ask for the outlier detector, such as SGDOneClassSVM, which is a linear model, LocalOutlierFactor, which is based on nearest neighbours, andIsolationForest based on bagging trees. We will focus on the LocalOutlierFactor, but I provide a code that you can easily modify to change a detector.\n    Okay, so how do we assess whether removing outliers will probably work for the test dataset? Well, let's cross-validate it. The point is that we carry on typical cross-validation, but in addition to train the model on the training subset, we train it again on the \"clean\" training subset, where \"clean\" means a subset without outliers. Subsequently, we collect results obtained for validation subsets for two types of models. This trained on full training subset and that trained on clean subset. Suppose we observe a reduction in terms of a given metric for the validation subset in all folds. In that case, that's probably a good method of removing outliers. And here is a crucial point. We should observe a reduction in all folds. When we observe a reduction in some folds but not in others, we rather should not remove outliers because the test dataset can resemble that one fold when removing outliers came with lower model performance.\n","metadata":{}},{"cell_type":"code","source":"def remove_outliers(data, detector):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(f\"'data' must be {pd.DataFrame!r} instance\")\n\n    result = detector.fit_predict(data)\n    outlier_ids = pd.Series(result == -1, index=data.index, dtype=bool)\n    data_ids = pd.Series(np.ones_like(data.index), index=data.index, dtype=bool)\n    \n    return data[~(outlier_ids & data_ids)]","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:50:17.874705Z","iopub.execute_input":"2023-12-24T18:50:17.875542Z","iopub.status.idle":"2023-12-24T18:50:17.881352Z","shell.execute_reply.started":"2023-12-24T18:50:17.875509Z","shell.execute_reply":"2023-12-24T18:50:17.880469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing Outliers\n\n    Since outlier detectors in scikit-learn predict $-1$ for the outlier sample, we create a mask and return the dataset without associated observations in the remove_outliers() function. Subsequently, below I create simple LGBM model which implements MAE loss pretty well compared to previously used random forests, and perform CV scheme I described earlier.\n","metadata":{}},{"cell_type":"code","source":"lgbm = DefaultLGBMRegressor()\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\ndetector = make_pipeline(\n    PowerTransformer(method=\"yeo-johnson\", standardize=True),\n    LocalOutlierFactor(),\n)\n\nhyperparameter = \"localoutlierfactor__contamination\"\nhyperparameter_values = [None] + np.arange(0.01, 0.15, 0.01).tolist()\nno_outliers_medae = {}\n\nfor k, (train_ids, valid_ids) in enumerate(kfold.split(X, y), start=1):\n    X_train, y_train = X.iloc[train_ids], y.iloc[train_ids]\n    X_valid, y_valid = X.iloc[valid_ids], y.iloc[valid_ids]\n\n    lgbm.fit(X_train, y_train)\n    default_medae = median_absolute_error(y_valid, lgbm.predict(X_valid))  # type:ignore\n\n    for hp_value in hyperparameter_values:\n        if hp_value is None:\n            no_outliers_medae[f\"0 - {k}\"] = default_medae\n            continue\n\n        detector.set_params(**{hyperparameter: hp_value})\n        X_no_outliers = remove_outliers(X_train, detector)\n        y_no_outliers = y_train[X_no_outliers.index]\n\n        lgbm.fit(X_no_outliers, y_no_outliers)\n        clean_medae = median_absolute_error(y_valid, lgbm.predict(X_valid))  # type:ignore\n        no_outliers_medae[f\"{hp_value} - {k}\"] = clean_medae","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:50:24.756013Z","iopub.execute_input":"2023-12-24T18:50:24.756367Z","iopub.status.idle":"2023-12-24T18:51:35.749886Z","shell.execute_reply.started":"2023-12-24T18:50:24.756340Z","shell.execute_reply":"2023-12-24T18:51:35.749028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detector_medae = pd.DataFrame({\"KEY\": no_outliers_medae.keys(), \"MedAE\": no_outliers_medae.values()})\ndetector_medae[[hyperparameter, \"Fold\"]] = detector_medae.KEY.str.split(\"-\", expand=True)\ndefault_medae = detector_medae[detector_medae[hyperparameter].astype(float) == 0].MedAE\n\nfig = px.line(\n    detector_medae,\n    x=hyperparameter,\n    y=\"MedAE\",\n    facet_row=\"Fold\",\n    facet_row_spacing=0.07,\n    color_discrete_sequence=COLOR_SCHEME[2:],\n    height=640,\n    width=840,\n    title=f\"Influence of '{hyperparameter}' Hyperparameter on MedAE in LGBM\",\n)\nfor fold, fold_default_medae in enumerate(default_medae):\n    fig.add_hline(\n        fold_default_medae,\n        annotation_text=f\"Default MedAE: {fold_default_medae:.3f}\",\n        annotation_position=\"bottom left\",\n        annotation_font_size=12,\n        line_width=1.5,\n        opacity=0.75,\n        line_dash=\"dot\",\n        line_color=COLOR_SCHEME[0],\n        row=len(default_medae) - fold,  # type:ignore\n    )\nfig.update_traces(line_width=2)\nfig.update_layout(margin_pad=10)\nfig.update_xaxes(tickformat=\".2f\", type=\"linear\")\nsave_and_show_fig(fig, \"outlier_detection\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-24T18:51:50.269296Z","iopub.execute_input":"2023-12-24T18:51:50.269836Z","iopub.status.idle":"2023-12-24T18:51:50.912292Z","shell.execute_reply.started":"2023-12-24T18:51:50.269802Z","shell.execute_reply":"2023-12-24T18:51:50.911303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing Outliers vs Performance\n\n    Unfortunately, removing outliers didn't reduce MedAE in all folds for any of the contamination values in the LocalOutlierFactor detector. I also checked IsolationForest and SGDOneClassSVM with several hyperparameter combinations, but the result is similar. Of course, the whole process depends on model combination. Perhaps this method works for other regressor, rather than LGBM.\n</p>","metadata":{}},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">5</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Modelling</span></b><a class=\"anchor\" id=\"modelling\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"markdown","source":"Stacked Ensemble with H2O and optuna","metadata":{}},{"cell_type":"code","source":"import h2o\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator\nfrom h2o.estimators import H2OXGBoostEstimator\nfrom h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\nimport optuna\nfrom tqdm import tqdm\nseed=42\n\n#initilize H2O cluster\nh2o.init()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:51:58.743093Z","iopub.execute_input":"2023-12-24T18:51:58.743804Z","iopub.status.idle":"2023-12-24T18:52:09.590060Z","shell.execute_reply.started":"2023-12-24T18:51:58.743770Z","shell.execute_reply":"2023-12-24T18:52:09.588915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_h2o=h2o.H2OFrame()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:52:14.655290Z","iopub.execute_input":"2023-12-24T18:52:14.655677Z","iopub.status.idle":"2023-12-24T18:52:14.660915Z","shell.execute_reply.started":"2023-12-24T18:52:14.655645Z","shell.execute_reply":"2023-12-24T18:52:14.659780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load data\n#H2O models can not take pandas dataframe so we need to convert from a Pandas Dataframe to H2O frame\ntrain_data_h2o=h2o.H2OFrame(train)\ntest_data_h2o=h2o.H2OFrame(test)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:52:18.086035Z","iopub.execute_input":"2023-12-24T18:52:18.086389Z","iopub.status.idle":"2023-12-24T18:52:20.907841Z","shell.execute_reply.started":"2023-12-24T18:52:18.086362Z","shell.execute_reply":"2023-12-24T18:52:20.906848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#H2O models can encode categorical feqtures automatically.They must be explicitly\n#converted to factor(categorical)data.\ncategorical_cols=[col for col in train_data_h2o if col.dtype=='object']\nfor col in categorical_cols:\n    train_data_h2o[col]=train_data_h2o[col].asfactor()\n    test_data_h2o[col]=test_data_h2o[col].asfactor()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:52:24.468611Z","iopub.execute_input":"2023-12-24T18:52:24.469341Z","iopub.status.idle":"2023-12-24T18:52:24.475574Z","shell.execute_reply.started":"2023-12-24T18:52:24.469307Z","shell.execute_reply":"2023-12-24T18:52:24.474657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split train data into train and validation using H2O functions\nsplits=train_data_h2o.split_frame(ratios=[0.8],seed=seed)\ntrain1=splits[0]\nval=splits[1]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:52:28.016090Z","iopub.execute_input":"2023-12-24T18:52:28.016480Z","iopub.status.idle":"2023-12-24T18:52:29.769513Z","shell.execute_reply.started":"2023-12-24T18:52:28.016449Z","shell.execute_reply":"2023-12-24T18:52:29.768370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Unlike Scikit-Learn models which take as input the values of the \n#features and the target, H2O models take as input the names of the features and the target.\nX = list(train1.columns)\ny = 'Hardness'\nX.remove(y)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:52:36.785036Z","iopub.execute_input":"2023-12-24T18:52:36.785408Z","iopub.status.idle":"2023-12-24T18:52:36.790041Z","shell.execute_reply.started":"2023-12-24T18:52:36.785379Z","shell.execute_reply":"2023-12-24T18:52:36.789009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:52:40.559454Z","iopub.execute_input":"2023-12-24T18:52:40.559859Z","iopub.status.idle":"2023-12-24T18:52:40.565548Z","shell.execute_reply.started":"2023-12-24T18:52:40.559828Z","shell.execute_reply":"2023-12-24T18:52:40.564108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">6</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Training Deep Neural Network(DNN) as base models</span></b><a class=\"anchor\" id=\"modelling\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"code","source":"dnn_models=[]\ndef objective(trial):\n    #params\n    num_hidden_layers=trial.suggest_int('num_hidden_layers',1,10)\n    hidden_layer_size=trial.suggest_int('hidden_layer_size',100,300,step=50)\n    params={\n        'hidden':[hidden_layer_size]*num_hidden_layers,\n        'epochs': trial.suggest_int('epochs',5,100),\n        'input_dropout_ratio':trial.suggest_float('input_dropout_ratio',0.1,0.3),\n        'l1':trial.suggest_float('l1',1e-5,1e-1,log=True),\n        'l2':trial.suggest_float('l2',1e-5,1e-1,log=True),\n        'activation':trial.suggest_categorical('activation',[\"maxoutwithdropout\", \"tanhwithdropout\", \"tanh\", \"rectifierwithdropout\", \"rectifier\", \"maxout\"])\n    }\n    \n    if params['activation'] in ['RectifierWithDropout',\"TanhWithDropout\",\"MaxoutWithDropout\"]:\n        hidden_dropout_rtio=trial.suggest_float('hidden_dropout_ratio',0.1,1.0)\n        params['hidden_dropout_ratios']=[hidden_dropout_rtio]*num_hidden_layers\n    #Train model\n    \n    model=H2ODeepLearningEstimator(**params,\n                                   standardize=True,\n                                   categorical_encoding='auto',\n                                   nfolds=5,\n                                   keep_cross_validation_predictions=True,\n                                   seed=seed\n                                  )\n    model.train(x=X,y=y,training_frame=train1)\n    \n    dnn_models.append(model)\n    \n    #CV score\n    cv_metrics_df=model.cross_validation_metrics_summary().as_data_frame()\n    cv_rmse_index=cv_metrics_df[cv_metrics_df['']=='rmse'].index\n    cv_rmse=cv_metrics_df['mean'].iloc[cv_rmse_index]\n    return cv_rmse\n\nstudy= optuna.create_study(direction='minimize')\n#study.optimize(objective,n_trials=20)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T18:54:50.725603Z","iopub.execute_input":"2023-12-24T18:54:50.726214Z","iopub.status.idle":"2023-12-24T20:18:10.650849Z","shell.execute_reply.started":"2023-12-24T18:54:50.726174Z","shell.execute_reply":"2023-12-24T20:18:10.648636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">7</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Training XGBoost and LightGBM base models</span></b><a class=\"anchor\" id=\"modelling\"></a> [â†‘](#top) ","metadata":{}},{"cell_type":"code","source":"xgboost_lightgbm_models=[]\ndef objective(trial):\n    \n    params={\n        'ntrees':trial.suggest_int('ntrees',50,5000),\n        'max_depth':trial.suggest_int('max_depth',1,9),\n        'min_rows':trial.suggest_int('min_rows',1,5),\n        'sample_rate':trial.suggest_float('sample_rate',0.8,1.0),\n        'col_sample_rate':trial.suggest_float('col_sample_rate',0.2,1.0),\n        'col_sample_rate_per_tree':trial.suggest_float('col_sample_rate_per_tree',0.5,1.0)\n    }\n    \n    grow_policy=trial.suggest_categorical('grow_policy',['depthwise','lossguide'])\n    \n    #add lightgbm specific params\n    if grow_policy=='lossguide':\n        tree_method='hist'\n        params['max_bins']=trial.suggest_int('max_bind',20,256)\n        params['max_leaves']=trial.suggest_int('max_leaves',31,1024)\n    \n    #add xgboost specific params\n    else:\n        tree_method='auto'\n        params['booster']=trial.suggest_categorical('booster',['gbtree','gblinear','dart'])\n        params['reg_alpha']=trial.suggest_float('reg_alpha',0.001,1)\n        params['reg_lambda']=trial.suggest_float('reg_lambda',0.001,1)\n        params['min_split_improvement']=trial.suggest_float('min_split_improvement',1e-10,1e-3,log=True)\n    \n    params['grow_policy']=grow_policy\n    params['tree_method']=tree_method\n    \n    #train model\n    model=H2OXGBoostEstimator(**params,\n                              learn_rate=0.1,\n                              categorical_encoding='auto',\n                              nfolds=5,\n                              keep_cross_validation_predictions=True,\n                              seed=seed\n                             )\n    model.train(x=X,y=y,training_frame=train1)\n    \n    #store models\n    xgboost_lightgbm_models.append(model)\n    \n    #CV\n    \n    cv_metrics_df=model.cross_validation_metrics_summary().as_data_frame()\n    cv_rmse_index=cv_metrics_df[cv_metrics_df['']=='rmse'].index\n    cv_rmse=cv_metrics_df['mean'].iloc[cv_rmse_index]\n    return cv_rmse\n\nstudy=optuna.create_study(direction='minimize')\nstudy.optimize(objective,n_trials=20)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-24T20:32:18.759894Z","iopub.execute_input":"2023-12-24T20:32:18.760692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">8</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Training the meta model on dnn and xgb models</span></b><a class=\"anchor\" id=\"modelling\"></a> [â†‘](#top)\n\n\nMost academic papers on stacked ensembles recommend choosing a simple linear-based algorithm for the meta-model. This is to avoid the meta-model overfitting to the predictions from the base models.","metadata":{}},{"cell_type":"code","source":"base_models= xgboost_lightgbm_models # + dnn_models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    #meta model params\n    meta_model_params={\n        'alpha':trial.suggest_float('alpha',0,1),\n        'family':trial.suggest_categorical('family',['gaussian','tweedie']),\n        'standardize':trial.suggest_categorical('standardize',['True','False']),\n        'non_negative':True\n        }\n    \n    ensemble=H2OStackedEnsembleEstimator(metalearner_algorithm='glm',\n                                        metalearner_params=meta_model_params,\n                                        metalearner_nfolds=5,\n                                        base_models=base_models,\n                                        seed=seed\n                                        )\n    ensemble.train(x=X,y=y,frame=train1)\n    \n    #CV\n    \n    cv_metrics_df=ensemble.cross_validation_metrics_summary().as_data_frame()\n    cv_rmse_index=cv_metrics_df[cv_metrics_df['']=='rmse'].index\n    cv_rmse=cv_metrics_df['mean'].iloc[cv_rmse_index]\n    return cv_rmse\n\nstudy=optuna.create_study(direction='minimize')\nstudy.optimize(objective,n_trials=20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">9</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Finally build best model from optimal parameters</span></b><a class=\"anchor\" id=\"modelling\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"code","source":"best_meta_model_params=study.best_params\nbest_ensemble=H2OStackedEnsembleEstimator(metalearner_algorithm='glm',\n                                         metalearner_params=best_meta_model_params,\n                                         base_models=base_models,\n                                         seed=seed)\nbest_ensemble.train(x=X,y=y,training_frame=train1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the best model\nbest_ensemble.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check validation score of best model\nensemble_val_rmse=best_ensemble.model_performance(val).rmse()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check performance of each base model on validation set","metadata":{}},{"cell_type":"code","source":"base_val_rmse=[]\nfor i in range(len(base_models)):\n    base_val_rmse=base_model[i].model_performance(val).rmse()\nmodels=['H2ODeepLearningEstimator']*len(dnn_models)+['H2OXGBoostEstimator']*len(xgboost_lightgbm_models)\n\nbase_val_rmse_df=pd.DataFrame([models,base_val_rmse]).T\nbase_val_rmse_df.columns=['model','rmse']\n\nbase_val_rmse_df=base_val_rmse_df.sort_values(by='val_rmse',ascending=True).reset_index(drop=True)\nbase_val_rmse_df.head(15)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">10</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Prediction</span></b><a class=\"anchor\" id=\"modelling\"></a> [â†‘](#top)","metadata":{}},{"cell_type":"code","source":"pred=best_ensemble.predict(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(\n    {\n        \"id\": test.index,\n        \"Hardness\": best_ensemble.predict(test).round(2),  # type:ignore\n    }\n).set_index(\"id\")\n\nsubmission.to_csv(\"submission.csv\")\nget_pretty_frame(submission.head(), precision=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">11</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\"> References</span></b><a class=\"anchor\" id=\"summary\"></a> [â†‘](#top)\n\nBelow are the sources I used to make this notebook. Many thanks to them.\n* https://www.kaggle.com/code/mateuszk013/playground-series-s3e25-mohs-hardness\n* https://medium.com/towards-data-science/stacked-ensembles-for-advanced-predictive-modeling-with-h2o-ai-and-optuna-8c339f8fb602","metadata":{}}]}